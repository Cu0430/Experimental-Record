# 4.15-4.18 再认识
## 4.15
数据也有问题！这个数据集是训练和测试分开的，而我一直是用的一个label文件再分开，之后的版本都用的是分开的train和test。
老师说可能是卷积层太多了，于是我把基础特征提取块和共享特征提取块都删掉，只留可学习字典（先）和resnet50（后）。\
预训练nan，考虑可能是对比学习的假负样本筛选中的聚类导致的，故删去。但还是nan。\
只留下resnet50、最后一层对比学习和假负样本筛选时，训练能到100%准确率，测试的OA是46.56%。说明可学习字典有问题。只改变多层对比学习时，训练100%，测试44%。说明可能冗余。\
同时还不想放弃v2，尝试过单独去掉对比学习损失、去掉假负样本筛选、去掉整个resnet只留模态补偿、去掉共享特征提取块等等，结果都很差，只有15%准确率。\
## 4.16 
突发奇想可以把模态补偿模块揉到resnet50中，都是特征提取块，为什么不用resnet50的浅层呢？于是尝试了一下，发现nan。\
Debug发现nan出现在模态补偿模块，可能是梯度爆炸，因为能训练第一个patch，之后再nan。于是把单头注意力改成八头，可以训练但是结果很差。\
之后把展开的尺寸对ms和pan分别合理设计，之前的设计使单头注意力压力太大，计算不过来。修改之后还是用单头，单层对比，无假负样本筛选，可以预训练！而且测试结果0A为37.789！比之前所有结果都好！\
说明之前可能确实是卷积层太多了。\
又考虑是不是下游训练样本过少，于是从1%加到5%，再到20%，OA分别提升到了45%和66%。
## 4.17
回到1%训练数据，在之前的基础上加假负样本筛选，提升至46%。\
之后就是调整超参数、学习率、分类头等，受FPN的启发，还加了模态感知的特征融合模块，把三层对比学习的特征融合，但基本都在40%-50%之间，没有大突破。
