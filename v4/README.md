# 4.22-4.27 反思
## 4.22-4.24
经过和老师的讨论，发现我自己说不清楚模态补偿模块怎么实现促进模态一致性和保留模态特异性信息的平衡。于是又恶补论文。\
把原来的mse loss改为各自的模态分类的交叉熵损失，让补偿后的特征更具模态判别性。相比不加，能提升5个百分点，但还是在30%左右。\
以后的版本都没再加融合部分，因为发现作用不大。先一个模块一个模块的抠。
## 4.26
又考虑到会不会是基线的问题，我又把基线进行了调整，去掉最大池化层，改了第一层的卷积核和stride。\
还考虑到数据的问题，下游训练一直是1%的训练数据。训练和测试分开之后，只有125个训练样本，而测试要200000左右个样本，根本不是1:99！于是用20%训练数据。\
**大突破！** \
经过上述改进，基线结果为AA:71.764-OA:70.477-KAPPA:66.601！这比之前所有结果都好！\
原来问题出现在最开始的部分......
## 4.27
在基线的基础上加上我的模态补偿模块之后，AA:74.4207-OA:71.8495-KAPPA:68.1575
# 5.8-5.14 再深挖
只是单纯的在后三层输出特征上做对比学习约束太浅了，进一步思考层与层之间应该形成渐进的形式，于是把历史的未对齐部分传到后续层进行指导。\
一开始在主干上修改结果并不好，比基线还低好几个点。后来在嵌入空间中做残差传递，好一些，但还是没基线高。\
之后发现没进行标准化，加上之后第一次跑的结果好（KAPPA:70.6725），后面再重复跑都没有第一次好。观察tensorboard结果，怀疑可能是初识残差不稳定导致误差传播放大，对比学习损失值出现较大波动。\
之后进行了一系列正则化改进，取得了目前的稳定的最好结果。\
但还需要进一步研究，个人认为目前的方案在每层的特定处理方面还可以精进，现在每层的处理有些雷同，怀疑是重复类似的处理导致冗余进而提升不大。
